<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Obtaining Uncertainty Estimates from Neural Networks Using TensorFlow Probability</title>
    <meta charset="utf-8" />
    <meta name="author" content="Sigrid Keydana" />
    <link rel="stylesheet" href="theme/rstudio.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Obtaining Uncertainty Estimates from Neural Networks Using TensorFlow Probability
### Sigrid Keydana
### Predictive Analytics World 2019

---


class: inverse, middle, center

# Neural networks are awesome ...


---

# Deep learning yields excellent results in e.g. 

&lt;br /&gt;

- Image recognition (classification, segmentation, object detection)

- Natural language processing (language models, translation, generation)

- Domain modeling (embeddings)

- Entity generation ("fakes"): Faces, text ...

- Tasks involving tabular data (e.g., recommender systems)

- Playing games (deep reinforcement learning)

---

class: inverse, middle, center

# So what are we missing?

---

# Networks output _point estimates_

&lt;br /&gt;


- A single numeric prediction 

- A single segmentation mask

- A single translation

- A single embedding

&lt;br /&gt;
 
But wait ... aren't there probabilities in there, _somewhere_?

---
# Name that animal

&lt;figure&gt;
&lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/p1.png"/&gt;
&lt;figcaption&gt;&lt;br /&gt;Image: Ben Tubby [CC BY 2.0 (https://creativecommons.org/licenses/by/2.0)]
https://upload.wikimedia.org/wikipedia/commons/3/30/Falkland_Islands_Penguins_35.jpg&lt;/figcaption&gt;
&lt;/figure&gt;

Let's ask the network ...


---
# What network?

... TensorFlow, but from R!

&lt;figure&gt;
    &lt;img src="dlwr.jpg" style = "float:left;" width= "35%"&gt;
&lt;/figure&gt;
&lt;figure&gt;
    &lt;img src="r-tf.png" style = "float:right;" width= "45%"&gt;
&lt;/figure&gt;


---
# R-TensorFlow

&lt;br /&gt; 
- [R bindings to TensorFlow](https://tensorflow.rstudio.com/) (subsumes Keras)

- TensorFlow-related infrastructure...
  - [Dataset preprocessing pipeline](https://github.com/rstudio/tfdatasets)
  - [TensorFlow datasets](https://github.com/rstudio/tfds)
  - [tfhub](https://github.com/rstudio/tfhub)
  - and more ...

- [_r-tensorflow_ ecosystem](https://github.com/r-tensorflow):
  - Model implementations, e.g. U-Net, WaveNet ... 
  - User-contributed packages for automation, tuning, ...

- blog: https://blogs.rstudio.com/tensorflow/

---
# With R-Keras, let's ask a pretrained model...


```r
library(tensorflow)
library(keras)

model &lt;- application_mobilenet_v2()
probs &lt;- model %&gt;% predict(image)
imagenet_decode_predictions(probs)
```

&lt;br /&gt;

```
  class_name class_description        score
1  n02056570      king_penguin 0.9899597168
2  n01847000             drake 0.0011793933
3  n01798484   prairie_chicken 0.0002387235
4  n02058221         albatross 0.0002117234
5  n02071294      killer_whale 0.0001432021
```

---
# Let's do another one

&lt;figure&gt;
&lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/p2.png"/&gt;
&lt;figcaption&gt;&lt;br /&gt;Image: M. Murphy [Public domain]&lt;br /&gt;
https://upload.wikimedia.org/wikipedia/commons/2/22/RoyalPenguins3.JPG&lt;/figcaption&gt;
&lt;/figure&gt;


---
# So what is that?


```r
probs &lt;- model %&gt;% predict(image)
imagenet_decode_predictions(probs)
```

&lt;br /&gt;

```
  class_name class_description      score
1  n02051845           pelican 0.24614049
2  n02009912    American_egret 0.18564136
3  n02058221         albatross 0.06848499
4  n02012849             crane 0.04572001
5  n02009229 little_blue_heron 0.03902744
```
&lt;br /&gt;

### Hm... what really is that _score_?

---
# Stepping back: What's a neural network?


&lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/playground.png" width = "70%"/&gt;

---
# Zooming in: Weights and activations

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/perceptron.png" width = "60%"&gt;
    &lt;figcaption&gt;Source: https://en.wikipedia.org/wiki/Perceptron&lt;/figcaption&gt;
&lt;/figure&gt;


---

# Activation functions in regression


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 32, activation = "relu", input_shape = 7) %&gt;%
  # default activation = linear
  layer_dense(units = 1)
```

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/relu.png" width = "60%"&gt;
    &lt;figcaption&gt;&lt;b&gt;ReLU&lt;/b&gt; activation&lt;/figcaption&gt;
&lt;/figure&gt;


---

# Sigmoid activation for binary classification


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 32, activation = "relu", input_shape = 7) %&gt;%
  layer_dense(units = 1, activation = "sigmoid")
```

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/sigmoid.png" width = "60%"&gt;
    &lt;figcaption&gt;&lt;b&gt;Sigmoid&lt;/b&gt; activation&lt;/figcaption&gt;
&lt;/figure&gt;

---

# Softmax activation for multiple classification


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 32, activation = "relu", input_shape = 7) %&gt;%
  layer_dense(units = 10, activation = "softmax")
```

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/softmax_pre.png" style = "float:left;" width= "50%"&gt;
&lt;/figure&gt;
&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/softmax_post.png" style = "float:right;" width= "50%"&gt;
&lt;/figure&gt;

Before and after __softmax__ activation.

---

# So our "probability" is just a maximum likelihood estimate ...

&lt;br /&gt;

... how can we make this probabilistic?

---
class: inverse, middle, center

# Indirect ways: ensembling (model averaging), dropout ...


---
# Dropout uncertainty (Gal &amp; Ghahramani, 2016)&lt;sup&gt;1&lt;/sup&gt;

- _dropout_: stochastic removal of units in hidden (or input) layers

- habitually used as a regularization measure during training

- idea: use at test time and average predictions

- mathematically derived to be ["equivalent to Monte Carlo integration over a Gaussian process posterior approximation"](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)

&lt;br /&gt;

## Question: How to choose the dropout rate?


.footnote[
[1] Gal and Ghahramani (2016): Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning
]

---
# Dropout uncertainty, next level (Kendall &amp; Gal, 2017) &lt;sup&gt;1&lt;/sup&gt;&lt;sup&gt;2&lt;/sup&gt;

&lt;br /&gt; 

- let the network learn the dropout rate

- let the network learn the variance (account for _heteroscedasticity_)

- independently calculate _epistemic_ and _aleatoric_ uncertainty

.footnote[
[1] Kendall &amp; Gal (2017): What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?

[2] Gal, Hron, &amp; Kendall (2017): Concrete Dropout.
]

---
# Types of uncertainty: aleatoric vs. epistemic&lt;sup&gt;1&lt;/sup&gt;

- 

.footnote[
[1]
Hora, S. (1996) Aleatory and epistemic uncertainty in probability elicitation with an example from hazardous waste management.
]

---
class: inverse, middle, center

# Or just directly: Bayesian networks!

---

# The idea: Put distributions over the network's weights


&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/weight_uncertainty.png" width = "120%"&gt;
    &lt;figcaption&gt;Blundell et al. (2015): Weight Uncertainty in Neural Networks&lt;/figcaption&gt;
&lt;/figure&gt;


---
# Many great predecessors...

- McKay (1992): A practical Bayesian framework for backprop networks

- Hinton and van Camp (1993): Keeping neural networks simple by minimizing the description length of the weights

- R. Neal (1996): Bayesian learning for neural networks


---
# But gained more momentum rather recently...

- Graves (2011): Practical Variational Inference for Neural Networks

- Kingma et al. (2015): Variational dropout and the local reparameterization trick

- Blundell et al. (2015): Weight uncertainty in neural networks (a.k.a "Bayes by backprop")


---
# Yeah, but how?

&lt;br /&gt;

[TensorFlow Probability](https://www.tensorflow.org/probability/) (Python library on top of TensorFlow)

[tfprobability](https://rstudio.github.io/tfprobability/) (R package)

&lt;br /&gt;


```r
devtools::install_github("rstudio/tfprobability")

library(tfprobability)
install_tfprobability()
```


---
# tfprobability (1): Basic building blocks

#### Distributions


```r
d &lt;- tfd_binomial(total_count = 7, probs = 0.3)
d %&gt;% tfd_mean()
#&gt; tf.Tensor(2.1000001, shape=(), dtype=float32)
d %&gt;% tfd_variance()
#&gt; tf.Tensor(1.47, shape=(), dtype=float32)
d %&gt;% tfd_log_prob(2.3)
#&gt; tf.Tensor(-1.1914139, shape=(), dtype=float32)
```

#### Bijectors


```r
b &lt;- tfb_affine_scalar(shift = 3.33, scale = 0.5)
x &lt;- c(100, 1000, 10000)
b %&gt;% tfb_forward(x)
#&gt; tf.Tensor([  53.33  503.33 5003.33], shape=(3,), dtype=float32)
```



---
# tfprobability (2): Higher-level modules

&lt;br /&gt;

- Keras layers 

- Markov Chain Monte Carlo (Hamiltonian Monte Carlo, NUTS)

- Variational inference

- State space models

- GLMs

---
class: inverse, middle, center

# How does that help us?


---
# With TFP, neural network layers can be _distributions_

&lt;br /&gt;
A network that has a multivariate normal distribution as output


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = params_size_multivariate_normal_tri_l(d)) %&gt;%
  layer_multivariate_normal_tri_l(event_size = d)

log_loss &lt;- function (y, model) - (model %&gt;% tfd_log_prob(y))

model %&gt;% compile(optimizer = "adam", loss = log_loss)

model %&gt;% fit(
  x,
  y,
  batch_size = 100,
  epochs = 1,
  steps_per_epoch = 10
)
```

---
# More distribution layers

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/distribution_layers.png" width = "100%"&gt;
    &lt;figcaption&gt;Complete list: &lt;a href="https://rstudio.github.io/tfprobability/reference/index.html#section-keras-layers-distribution-layers"&gt;Distribution layers&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

---
# Outputting a distribution: Aleatoric uncertainty


Instead of a single unit (of a _dense_ layer), we output a __normal distribution__:


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 8, activation = "relu") %&gt;%
  layer_dense(units = 2, activation = "linear") %&gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + tf$math$softplus(x[, 2, drop = FALSE])
               )
  )

negloglik &lt;- function(y, model) - (model %&gt;% tfd_log_prob(y))
model %&gt;% compile(
  optimizer = optimizer_adam(lr = 0.01),
  loss = negloglik
)
model %&gt;% fit(x, y, epochs = 1000)
```

---
# Aleatoric uncertainty ~ learned spread in the data


```r
yhat &lt;- model(tf$constant(x_test))
mean &lt;- yhat %&gt;% tfd_mean()
sd &lt;- yhat %&gt;% tfd_stddev()
```

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/aleatoric.png" width = "80%"&gt;
&lt;/figure&gt;

---
# Placing a distribution over the weights: Epistemic uncertainty


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense_variational(
    units = 1,
    make_posterior_fn = posterior_mean_field,
    make_prior_fn = prior_trainable,
    kl_weight = 1 / n
  ) %&gt;%
  layer_distribution_lambda(
    function(x) tfd_normal(loc = x, scale = 1)
    )

negloglik &lt;- function(y, model) - (model %&gt;% tfd_log_prob(y))
model %&gt;% compile(
  optimizer = optimizer_adam(lr = 0.1),
  loss = negloglik
)
model %&gt;% fit(x, y, epochs = 1000)
```

---
# Epistemic uncertainty ~ model uncertainty

Every prediction uses a different _sample from the weight distributions_!


```r
yhats &lt;- purrr::map(1:100, function(x) model(tf$constant(x_test)))
```

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/epistemic.png" width = "80%"&gt;
&lt;/figure&gt;

---
# Posterior weights are computed using variational inference

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/blei_vi.png" width = "80%"&gt;
    &lt;figcaption&gt;Source: &lt;a href="https://media.nips.cc/Conferences/2016/Slides/6199-Slides.pdf"&gt;David Blei, Rajesh Ranganath, Shakir Mohamed: Variational Inference:Foundations and Modern Methods. NIPS 2016 Tutorial·December 5, 2016.&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

---
# Epistemic and aleatoric uncertainty in one model


```r
model &lt;- keras_model_sequential() %&gt;%
  layer_dense_variational(
    units = 2,
  ) %&gt;%
  layer_distribution_lambda(function(x)
    tfd_normal(loc = x[, 1, drop = FALSE],
               scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
               )
    )

yhats &lt;- purrr::map(1:100, function(x) model(tf$constant(x_test)))
means &lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %&gt;% abind::abind()
sds &lt;-
  purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %&gt;% abind::abind()
```


---
# Main challenge now is how to display ...

Each line is one draw from the posterior weights; each line has its own standard deviation.

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/both.png" width = "70%"&gt;
&lt;/figure&gt;

More background: https://blogs.rstudio.com/tensorflow/posts/2019-06-05-uncertainty-estimates-tfprobability/

---
class: inverse, middle, center




# Other ways of modeling uncertainty with TFP


---
# Variational autoencoders: Informative latent codes

&lt;figure&gt;
    &lt;img src="https://raw.githubusercontent.com/skeydan/whyR2019/master/vae.png" width = "80%"&gt;
    &lt;figcaption&gt;Source: https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html&lt;/figcaption&gt;
&lt;/figure&gt;


---
# Variational autoencoders with tfprobability


```r
encoder_model &lt;- keras_model_sequential() %&gt;%
  [...] %&gt;%
  layer_multivariate_normal_tri_l(event_size = encoded_size) %&gt;%
  # pass in the prior of your choice ...
  # can use exact KL divergence or Monte Carlo approximation
  layer_kl_divergence_add_loss([...])

decoder_model &lt;- keras_model_sequential() %&gt;%
  [...] %&gt;%
 layer_independent_bernoulli([...])

vae_model &lt;- keras_model(inputs = encoder_model$inputs,
                         outputs = decoder_model(encoder_model$outputs[1]))
vae_loss &lt;- function (x, rv_x) - (rv_x %&gt;% tfd_log_prob(x))
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
